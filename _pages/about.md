---
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a french researcher interning at Inria before my PhD, advised by Loucas Pillaud-Vivien and Francis Bach. My research interest are in understanding deep-learning systems through mathematical/theoretic tools, in particular during training and deployment: **How does a model train ?** **What computation does a trained model do ?** Here is a list of resources illustrating these questions :
* [Singular Learning Theory](https://www.lesswrong.com/s/mqwA5FcL6SrHEQzox) : wild maths that link singular algebra to bayesian machine learning.
* [Mechanistic interpretability](https://www.neelnanda.io/mechanistic-interpretability) : one can try to recover algorithms learned by foundational models to understand various tasks like memorization / translation / elementary algebra.
* [Superposition](https://transformer-circuits.pub/2022/toy_model/index.html), [Grokking](https://www.neelnanda.io/grokking-paper) and associated phenomenons.
* [Implicit biases](https://arxiv.org/abs/2206.00939) : when neural networks interpolate data, many interpolating solutions exists, and I am interested in understanding the properties of the one found by gradient descent-like algorithms.

## About me
I volunteer for important, meaningful and fun associations. I am currently involved in 
* [EffiSciences](https://www.effisciences.org/fr), where I helped develop and promote the idea of *Recherche Impliquée* (RI), (close to *Impactful Research*), which says that research can and should try to have a positive impact on the world. EffiSciences has a lot of different projects on which I contributed since 2022 in RI, biosecurity and safe AIs. We recently wrote a report on impactful research that you can find here.
* [CeSIA](https://www.securite-ia.fr/), where I helped teach Machine Learning at [ML4G bootcamps](https://www.ml4good.org/), Interpretability research at the [Turing seminar](https://www.master-mva.com/cours/seminaire-turing/) ([slides](Slides/Turing seminar MVA 24_25 - Interpretability .pdf)), and helped for other field building event in the AI-safety community.

**Other stuff**: I like to read and sometime post on [LessWrong](https://www.lesswrong.com/users/wcargo). My favorite sport is {biking, running, weight-lifting, bjj, climbing}. I love to discuss philosophy, with some favorite topics being : Absurdism (Camus), Consciousness, Language (Wittgenstein), Moral.

My [**CV**](CV copie.pdf).

## Papers

Not much, but here is my first preprint !
* *Memorization in Attention-only Transformers*, Léo Dana, Muni Sreenivas Pydi, Yann Chevaleyre, preprint for AISTATS 2025. (Work done at Paris-Dauphine University.)
